<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>
 -->
  <title>Nikita Dhawan</title>
  
  <meta name="author" content="Nikita Dhawan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Nikita Dhawan</name>
              </p>
              <br>
              <p>I am a PhD student at the University of Toronto and the Vector Institute, supervised by Professors <a href="https://www.cs.toronto.edu/~cmaddis/">Chris Maddison</a> and <a href="https://www.cs.toronto.edu/~rgrosse/">Roger Grosse</a>. I completed my Bachelors in Computer Science and Applied Math at UC Berkeley, where I enjoyed working with Professor <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>. 
                 <!-- and <a href="http://marvinzhang.com/">Marvin Zhang</a>. -->
              </p>
              <br>
              <p style="text-align:center">
                <a href="mailto:nikita@cs.toronto.edu">Email</a> &nbsp/&nbsp
<!--                 <a href="cv.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/nikita-dhawan-7a4a29149/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4D_8pTEAAAAJ&hl=en&oi=ao">Google Scholar</a> 
<!--                 &nbsp/&nbsp -->
<!--                 <a href="https://www.facebook.com/barron.jon">Facebook</a> &nbsp/&nbsp -->
<!--                 <a href="https://twitter.com/jon_barron">Twitter</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Nikita.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Nikita.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in developing algorithms for reliable and trustworthy machine learning, with a particular focus on representation learning, self-supervision and continual learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>   
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fsd.png" alt="avid">
            </td>
            <td style="width:80%;vertical-align:middle">
              <papertitle>Efficient Parametric Approximations of Neural Network Function Space Distance</papertitle>
              <br>
              <strong>Nikita Dhawan</strong>,      
              <a href="https://www.cs.toronto.edu/~huang/">Sicong (Sheldon) Huang</a>,
              <a href="https://www.juhanbae.com/">Juhan Bae</a>,
              <a href="http://www.cs.toronto.edu/~rgrosse/">Roger Grosse</a>
              <br>
              <em>ICML</em>, 2023  
              <br>
<!--               <a href="https://sites.google.com/view/adaptive-risk-minimization">website</a> / -->
              <a href="https://arxiv.org/abs/2302.03519">arXiv</a> 
<!--               <a href="https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/">blog</a> -->
              <p> We consider a specific case of approximating the function space distance (FSD) over the 
                training set, i.e. the average distance between the outputs of two ReLU neural networks, based on approximating the architecture as a 
                linear network with stochastic gating. Despite requiring only one 
                parameter per unit of the network, our parametric approximation is competitive with state-of-the-art nonparametric approximations 
                with larger memory requirements, when applied to continual learning and influence function estimation. </p>
            </td>         
          </tr>
          
          <tr>   
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/data_inf.png" alt="avid">
            </td>
            <td style="width:80%;vertical-align:middle">
              <papertitle>Dataset Inference for Self-Supervised Models</papertitle>
              <br>
              <a href="https://adam-dziedzic.com/">Adam Dziedzic</a>,
              <a href="https://www.cs.toronto.edu/~haonand/">Haonan Duan</a>,
              <a href="https://ahmadkaleem123.github.io/">Muhammad Ahmad Kaleem</a>,
              <strong>Nikita Dhawan</strong>,      
              <a href="https://cleverhans-lab.github.io/members/jonas.html">Jonas Guan</a>,
              <a href="https://www.papernot.fr/">Yannis Cattan</a>,
              <a href="https://franziska-boenisch.de/">Franziska Boenisch</a>,
              <a href="https://www.papernot.fr/">Nicolas Papernot</a>
              <br>
              <em>NeurIPS</em>, 2022  
              <br>
<!--               <a href="https://sites.google.com/view/adaptive-risk-minimization">website</a> / -->
              <a href="https://arxiv.org/abs/2209.09024">arXiv</a> 
<!--               <a href="https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/">blog</a> -->
              <p> We introduce a new dataset inference defense for self-supervised models, which uses the intuition that the log-likelihood of an encoder's output representations is higher on the victim's training data than 
                on test data if it is stolen from the victim, but not if it is independently trained. Our extensive empirical results in the vision 
                domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.</p>
            </td>         
          </tr>
          
          <tr>   
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ssl_steal.png" alt="avid">
            </td>
            <td style="width:80%;vertical-align:middle">
              <papertitle>On the Difficulty of Defending Self-Supervised Learning against Model Extraction</papertitle>
              <br>
              <a href="https://adam-dziedzic.com/">Adam Dziedzic</a>,
              <strong>Nikita Dhawan</strong>,
              <a href="https://ahmadkaleem123.github.io/">Muhammad Ahmad Kaleem</a>,
              <a href="https://cleverhans-lab.github.io/members/jonas.html">Jonas Guan</a>,
              <a href="https://www.papernot.fr/">Nicolas Papernot</a>
              <br>
              <em>ICML</em>, 2022  
              <br>
<!--               <a href="https://sites.google.com/view/adaptive-risk-minimization">website</a> / -->
              <a href="https://arxiv.org/abs/2205.07890">arXiv</a> 
<!--               <a href="https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/">blog</a> -->
              <p>Recently, ML-as-a-Service providers have commenced offering trained self-supervised models over inference APIs, which transform 
                user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs 
                both make black-box extraction a realistic security threat. We explore model stealing by constructing several novel attacks and evaluating 
                existing classes of defenses.</p>
            </td>         
          </tr>
          
          <tr>   
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/arm.png" alt="avid">
            </td>
            <td style="width:80%;vertical-align:middle">
              <papertitle>ARM: A Meta-Learning Approach for Tackling Group Shift</papertitle>
              <br>
              <a href="http://marvinzhang.com/">Marvin Zhang*</a>,
              <a>Henrik Marklund*</a>,
              <strong>Nikita Dhawan*</strong>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
              <br>
              <em>NeurIPS</em>, 2021  
              <br>
              <a href="https://sites.google.com/view/adaptive-risk-minimization">website</a> /
              <a href="https://arxiv.org/abs/2007.02931">arXiv</a> 
<!--               <a href="https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/">blog</a> -->
              <p>Machine learning systems are regularly tested under distribution shift, in real-life applications. In this work, we consider the setting where 
                the training data are structured into groups and test time shifts correspond to changes in the group distribution. We propose to use ideas from 
                meta-learning to learn models that are adaptable, and introduce the framework of adaptive risk minimization (ARM), a formalization of this setting.</p>
            </td>         
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avid.png" alt="avid">
            </td>
            <td style="width:80%;vertical-align:middle">
              <papertitle>AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos</papertitle>
              <br>
              <a href="https://lauramsmith.github.io/">Laura Smith</a>,
              <strong>Nikita Dhawan</strong>,
              <a href="http://marvinzhang.com/">Marvin Zhang</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <br>
              <em>RSS</em>, 2020  
              <br>
              <a href="https://sites.google.com/view/rss20avid">website</a> /
              <a href="https://arxiv.org/abs/1912.04443">arXiv</a> /
              <a href="https://bair.berkeley.edu/blog/2019/12/13/humans-cyclegan/">blog</a>
              <p>Humans can learn from watching others, imagining how they would perform the task themselves, and then practicing on their own.
                Can robots do the same? We adopt a similar strategy of imagination and practice in this project to solve complex, long-horizon tasks, 
                like operating a coffee machine or getting objects from within a closed drawer.</p>
            </td>
          </tr> 
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <!-- <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="images/dcs_logo.png" alt="dcs">
            </td> -->
            <td style="width:100%;vertical-align:middle">
<!--             <td width="75%" valign="center"> -->
              <papertitle>Student Researcher</papertitle> 
              <br>
              <em>Google</em>, April 2023 -- Present
              <br>
              <p>Hosted by Nicole Mitchell and Karolina Dziugaite.</p>
              <!-- <br>
              <br>
              <papertitle>EECS 126: Probability and Random Processes</papertitle> <a href="https://inst.eecs.berkeley.edu/~ee126/fa20/">Fall 2020</a>, <a href="https://inst.eecs.berkeley.edu/~ee126/sp20/">Spring 2020</a> (UC Berkeley)
              <br>
              <br>
              <papertitle>EECS 229A: Information Theory and Coding</papertitle> <a href="https://sites.google.com/view/eecs229a-fall20">Fall 2020</a> (UC Berkeley) -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="images/dcs_logo.png" alt="dcs">
            </td>
            <td style="width:80%;vertical-align:middle">
<!--             <td width="75%" valign="center"> -->
              <papertitle>CSC 311: Introduction to Machine Learning</papertitle> <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc311_f21/">Fall 2021</a> (University of Toronto)
              <br>
              <br>
              <papertitle>EECS 126: Probability and Random Processes</papertitle> <a href="https://inst.eecs.berkeley.edu/~ee126/fa20/">Fall 2020</a>, <a href="https://inst.eecs.berkeley.edu/~ee126/sp20/">Spring 2020</a> (UC Berkeley)
              <br>
              <br>
              <papertitle>EECS 229A: Information Theory and Coding</papertitle> <a href="https://sites.google.com/view/eecs229a-fall20">Fall 2020</a> (UC Berkeley)
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
